{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d2b1071",
   "metadata": {},
   "source": [
    "# TP Majeures Science des données\n",
    "\n",
    "### Gérér l'affichage des courbes\n",
    "\n",
    "On va utiliser pyplot du module matplotlib pour afficher les courbes et les graphiques\n",
    "\n",
    "La commande *%matplotlib inline* fait en sorte que les courbes apparaissent dans le notebook.\n",
    "\n",
    "Si vous voulez sauvegarder les courbes sans les afficher, il faut ajouter la commande *matplotlib.use('Agg')* entre les 2 commandes suivantes :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a5435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utiliser matplotlib\n",
    "%matplotlib inline\n",
    "#matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a22a2b",
   "metadata": {},
   "source": [
    "## Machines à Vecteurs de Support (SVM)\n",
    "\n",
    "Si anaconda3 n'est pas installé, il nous faut d'abord installer les modules nécessaires. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eb66da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!pip3 -q install sklearn\n",
    "!pip3 -q install matplotlib\n",
    "!pip3 -q install seaborn\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dcb2ec",
   "metadata": {},
   "source": [
    "## Première partie : prise en main des SVM\n",
    "Cette partie est librement inspirée du travail de Jake VenderPlas, auteur du livre [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do). Son [GitHub](https://github.com/jakevdp/PythonDataScienceHandbook) (en anglais) regorge de fichiers utiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826ea0b3",
   "metadata": {},
   "source": [
    "Dans un premier temps, on va générer des données jouets, linéairement séparables :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d6aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Un petit environment qui donne de meilleurs graphes\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "# fonction sklearn pour générer des données simples\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "\n",
    "# Affichage des données\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='prism');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648768ae",
   "metadata": {},
   "source": [
    "### SVM linéaire (Séparateur à vaste marge)\n",
    "On va commencer par apprendre un SVM linéaire (sans noyau) à l'aide de scikit-learn :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf6d67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import de la classe - qui s'appelle SVC et pas SVM...\n",
    "from sklearn.svm import SVC\n",
    "#Définition du modèle\n",
    "model = SVC(kernel='linear', C=1E10)\n",
    "#Apprentissage sur les donnée\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77054f58",
   "metadata": {},
   "source": [
    "On va utiliser une fonction d'affichage qui va bien, où tout ce qui est nécessaire est affiché."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def affiche_fonction_de_decision(model, ax=None, plot_support=True):\n",
    "    \"\"\"Affiche le séparateur, les marges, et les vecteurs de support d'un SVM en 2D\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # création de la grille pour l'évaluation\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # affichage de l'hyperplan et des marges\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # Affichage des vecteurs de support\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, facecolors='none', edgecolor='black');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9bdf3b",
   "metadata": {},
   "source": [
    "Voyons ce que cela donne sur notre séparateur linéaire à vaste marge :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb8b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='prism')\n",
    "affiche_fonction_de_decision(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3619698",
   "metadata": {},
   "source": [
    "Sur ce graphe, on voit le séparateur (ligne pleine), les vecteurs de support (points entourés) et la marge (matérialisée par des lignes discontinues). On a ici le séparateur qui maximise la marge. Scikit-learn nous permet, après apprentissage, de récupérer les vecteurs de supports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be118e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c5382e",
   "metadata": {},
   "source": [
    "Seules trois données sont utiles pour classer de nouvelles données. On peut s'en assurer en rajoutant des données sans changer le modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee742b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2, y2 = make_blobs(n_samples=200, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "\n",
    "model2 = SVC(kernel='linear', C=1E10)\n",
    "model2.fit(X2, y2)\n",
    "\n",
    "plt.scatter(X2[:, 0], X2[:, 1], c=y2, s=50, cmap='prism')\n",
    "affiche_fonction_de_decision(model2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79119132",
   "metadata": {},
   "source": [
    "## SVM non linéaire\n",
    "Comme vu en cours, la puissance des séparateurs linéaires est limitée (à des données linéairement séparables). Mais il est possible de contourner cette limitation par l'utilisation de noyaux.\n",
    "\n",
    "On va commencer par générer des données non-linéairement séparables, puis on apprend un classifieur SVM linéaire et on affiche le résultat :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7bd192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "X, y = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "clf = SVC(kernel='linear').fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='Dark2')\n",
    "affiche_fonction_de_decision(clf, plot_support=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006e7f0",
   "metadata": {},
   "source": [
    "Clairement notre apprentissage de séparateur linéaire a échoué...\n",
    "\n",
    "On va manuellement ajouter une troisième dimension *z* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e5337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.exp(-(X ** 2).sum(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe50c3",
   "metadata": {},
   "source": [
    "On peut afficher les données augmentées et se rendre compte qu'elles sont linéairement séparables dans ce nouvel espace de dimension plus grande :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dee82fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "ax = plt.subplot(projection='3d')\n",
    "ax.scatter3D(X[:, 0], X[:, 1], z, c=y, s=50, cmap='Dark2')\n",
    "ax.view_init(elev=30, azim=30)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13a0da6",
   "metadata": {},
   "source": [
    "Le plan définit par z=0.7 (par exemple) sépare les 2 classes parfaitement.\n",
    "\n",
    "Bien entendu, la projection en plus grande dimension est capitale, et en choisissant un autre calcul pour *z* on aurait probablement obtenu des données non linéairement séparables.\n",
    "\n",
    "Et s'il fallait faire effectivement la projection, cela limiterait drastiquement la dimension de l'espace de plongement ainsi que le nombre de données traitables. C'est pourquoi l'utilisation de noyaux (kernels en anglais) est d'une grande efficacité.\n",
    "\n",
    "En Scikit-Learn, il suffit de modifier le paramètre *kernel* : jusqu'à présent, nous avons utilisé 'linear' comme valeur. On peut par exemple utiliser *rbf* pour 'radial basis function', le noyau gaussien (celui qui transforme notre espace de description initial vers le 3D avec $z$ précédent), et il nous reste à trouver la bonne valeur du paramètre :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6870f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', C=1E10)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c46fecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='Dark2')\n",
    "affiche_fonction_de_decision(clf) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c193ba64",
   "metadata": {},
   "source": [
    "**Exercice** : exécuter les instructions permettant un apprentissage avec un autre noyau -- pour un plongement dans un autre espace (par exemple noyau polynomial de degré 5), et la visualisation du séparateur. Vous devriez constater que ce n'est pas un noyaux très adapté !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a52299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a vous\n",
    "clf = SVC(kernel='poly', degree=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb88ef",
   "metadata": {},
   "source": [
    "On voit ici que le séparateur (et la marge associée) ne sont pas linéaire dans l'espace des données, mais qu'ils peuvent s'y représenter sans difficulté.\n",
    "\n",
    "Notons aussi que le nombre de vecteurs de support reste très petit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471c3095",
   "metadata": {},
   "source": [
    "### SVM à marge douce\n",
    "\n",
    "Il est aussi possible que le problème soit linéairement séparable (dans la dimension initiale des données ou dans un plongement) mais que le bruit (=la mauvaise qualité des données) empêche l'apprenant de trouver un séparateur.\n",
    "\n",
    "On utilise alors ce que l'on appelle un classifieur à marge douce : on autorise certains points à être dans la marge, voire du mauvais côté de l'hyperplan. C'est le role du paramètre *C* : pour des grosses valeurs, on est quasiment en marge dure, mais plus *C* prend des petites valeurs, plus les marges deviennent permissibles.\n",
    "\n",
    "On va prendre des données qui se chevauchent un peu : (à ce stade, il est important de comprendre la spécificité des données que l'on génére ci-après: en cas de doute appelez votre enseignant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f7942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=1.2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='jet');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fcb347",
   "metadata": {},
   "source": [
    "On joue alors avec la valeur de *C*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef28ed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for axi, C in zip(ax, [10.0, 0.1]):\n",
    "    model = SVC(kernel='linear', C=C).fit(X, y)\n",
    "    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='jet')\n",
    "    affiche_fonction_de_decision(model, axi)\n",
    "    axi.scatter(model.support_vectors_[:, 0],\n",
    "                model.support_vectors_[:, 1],\n",
    "                s=300, lw=1, facecolors='none');\n",
    "    axi.set_title('C = {0:.1f}'.format(C), size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c827541",
   "metadata": {},
   "source": [
    "## Paramétrer (tuner) un SVM\n",
    "Tous les noyaux sont paramétrés : il est question ici d'étudier l'impact d'un (hyper)paramètre sur la qualité de l'apprentissage.\n",
    "Pour cela, on va générer des données qui ne sont pas linéairement séparables :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04354536",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=200, centers=2,\n",
    "                  random_state=0, cluster_std=1.3)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='prism')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a633c0ef",
   "metadata": {},
   "source": [
    "On va étudier 2 noyaux différents\n",
    "- le noyau polynomial (*kernel='poly'*) qui a 2 paramètres, *degree* qu'il faut faire varier entre 2 et 6 (au minimum), et *C* (lié à la 'douceur' de la marge)\n",
    "- le noyau gaussien (*kernel='rbf'*) qui a aussi 2 paramètres, *gamma*, qu'il faut faire varier de 1 à 0.01, et *C* \n",
    "\n",
    "A chaque fois, en plus de l'affichage des séparateurs, et de l'estimation de l'erreur, il serait intéressant de regarder combien de vecteurs de support le classifieur appris a besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730fa7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exemple avec le noyau gaussien et des valeurs pour gamma et C\n",
    "clf = SVC(kernel='rbf', gamma=0.01, C=1E10)\n",
    "clf.fit(X, y)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='prism')\n",
    "affiche_fonction_de_decision(clf) \n",
    "print(\"Nombre de vecteurs de support (sur 200 données) :\", len(clf.support_vectors_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32269784",
   "metadata": {},
   "source": [
    "A vous de jouer ! (pour chaque noyau, faire varier les hyper-paramètres dans les intervalles mentionnés, et pour chaque couple d'hyper-paramètres : afficher la frontière de décison, le nombre de vecteurs supports du modèle (le plus petit est le mieux), et le score estimé sur un échantillon test de taille 100 généré de la même façon que l'échantillon d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f50d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a vous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6fd54d",
   "metadata": {},
   "source": [
    "## Deuxième partie : un traitement (presque) complet\n",
    "### Préparation des données\n",
    "Nous allons utiliser un jeu de données réel - tiré de *Tsanas & Xifara : Accurate quantitative estimation of energy performance of residential buildings using statistical machine learning tools, Energy and Buildings, Vol. 49, pp. 560-567, 2012* - qui vous est fourni avec l'énoncé."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159e375f",
   "metadata": {},
   "source": [
    "Les 8 premières colonnes correspondent aux attributs descriptifs et les deux dernières, aux charges de chauffage et de climatisation (dans cet ordre).\n",
    "Pour les utiliser en Python, vous pourrez vous servir du code suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b493dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"./data.csv\")\n",
    "X = data[:,:-2]\n",
    "Y = data[:,-2:]\n",
    "Yheat = Y[:,0]\n",
    "Ycool = Y[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d96f887",
   "metadata": {},
   "source": [
    "Le problème initial, tel que présenté ici, est un problème de régression. Nous allons d'abord le transformer en problème de classification. Par une méthode de clustering, on veut répartir les charges de chauffage et de climatisation en 3 classes : faibles, moyennes, élevées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f89f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# La suite ? il s'agit de définir un classifieur du k-means avec k=3 \n",
    "# et d'utiliser la méthode 'fit' sur les 2 ensembles de valeurs Y\n",
    "\n",
    "# Le seul trick : les Y sont des vecteurs et les classifieurs sklearn ont besoin d'array :\n",
    "# il faut les reshaper : Yheat_vector = Yheat.reshape(-1,1)\n",
    "\n",
    "# Après apprentissage du kmeans, les classes des données utilisées sont stockées dans mon_classifieur.labels_\n",
    "\n",
    "# Concaténez les vecteurs Yheat et Ycool pour créer une seule matrice Y\n",
    "Y_matrix = np.column_stack((Yheat, Ycool))\n",
    "\n",
    "# Définissez un classifieur K-Means avec 3 clusters (k=3)\n",
    "kmeans_classifier = KMeans(n_clusters=3)\n",
    "\n",
    "# Utilisez la méthode 'fit' pour entraîner le modèle sur la matrice Y\n",
    "kmeans_classifier.fit(Y_matrix)\n",
    "\n",
    "# Après l'apprentissage du K-Means, les classes des données utilisées sont stockées dans kmeans_classifier.labels_\n",
    "kmeans_classifier.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24af202e",
   "metadata": {},
   "source": [
    "### Apprentissage\n",
    "Nous voulons comparer plusieurs méthodes d'apprentissage :\n",
    "\n",
    "1. Les arbres de décision  (*DecisionTreeClassifier* de la classe *sklearn.tree*, hyperparamètre à régler : *max_depth*)\n",
    "2. SVM à noyau gaussien  (*SVC* avec *kernel='rbf'* de la classe *sklearn.svm*, hyperparamètre à régler : *gamma*)\n",
    "3. SVM à noyau polynomial (*SVC* avec *kernel='poly'* de la classe *sklearn.svm*, hyperparamètre à régler : *degree*)\n",
    "\n",
    "Ecrivez le code permettant de :\n",
    "1. Séparer les données en un échantillon d'apprentissage et un échantillon de test (80/20)\n",
    "2. Sélectionner les meilleurs valeurs des hyperparamètres sur l'échantillon d'apprentissage par validation croisée en utilisant 10 folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a552a1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# A vous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07734ac2",
   "metadata": {},
   "source": [
    "## Analyse des résultats\n",
    "Afficher sur une courbe les scores de chacun des algorithmes avec la meilleure valeur d'hyperparamètre possible sur l'échantillon de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ba619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# à vous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725ced9c",
   "metadata": {},
   "source": [
    "Pour chacune des méthodes, pour chaque meilleur hyperparamètre, calculer l'intervalle à 95% de confiance auquel le score doit appartenir en utilisant les résultats de la validation croisée. Si vous ne vous souvenez plus de comment on calcule un intervalle de confiance, vous pouvez consulter : https://fr.wikihow.com/calculer-un-intervalle-de-confiance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0f7f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A vous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454e6783",
   "metadata": {},
   "source": [
    "Quelle méthode est la meilleure pour prédire la classe de frais de chauffage ? De frais de climatisation ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98700f4c",
   "metadata": {},
   "source": [
    "# Bonus : Travail à réaliser\n",
    "\n",
    "Reproduisez pour les datasets suivants:\n",
    "- [Iris](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris)\n",
    "- [Digits](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits) (en utilisant les données complètes)\n",
    "\n",
    "\n",
    "les expérimentations suivantes:\n",
    "\n",
    "- Mise au point de plusieurs types de classifieurs (Perceptron, régression logistique, SVM, Knn). Pour chacun de ces types de classifieurs vous devrez :\n",
    " - Définir les hyper-paramètres à faire varier.\n",
    " - Evaluer et selectionner par Grid-Search l'ensemble des configurations possibles, en utilisant la Validation Croisée à 3 plis pour l'évaluation de la performance en généralisation. Vous pourrez vous inspirer d'un code tel que [celui-ci](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py) pour boucler sur les datasets et/ou les classifieurs.\n",
    "- ### Ecrire sous forme d'un tableau récapitulatif les performances respectives (les meilleures obtenues) par chacun des modèles sur chacun des jeux de données (sur le test set).\n",
    "- Donner des conclusions sur les résultats obtenus quant à la performance, la stabilité, la robustesse des familles de classifieurs utilisées, et les paramètres optimaux de chaque type de modèle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fe2453",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
